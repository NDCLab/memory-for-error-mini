# This script runs reliability analysis on mini_mfe data.
# Author: Kianoosh Hosseini at NDCLab @FIU (May-September 2022; https://Kianoosh.info; https://NDClab.com)
# Last Update: 2023-03-06 (YYYY-MM-DD)

library(dplyr)
library(stringr)
library(ggplot2)
library(splithalfr)


#Working directory should be the Psychopy experiment directory.
proje_wd <- "/home/khoss005/Documents/mini_mfe"
setwd(proje_wd)

input_raw_path <- paste(proje_wd, "sourcedata", "checked", "mini_mfe_psychopy", sep ="/", collapse = NULL) # input data directory
input_organized_path <- paste(proje_wd, "derivatives", "mini_mfe_psychopy", "csv_output", sep ="/", collapse = NULL) # input directory for data files generated by the organizer script!
output_path <- paste(proje_wd, "derivatives", "mini_mfe_psychopy", "stat_output", sep ="/", collapse = NULL) # output directory
flanker_csv_fileName <- "_mini_mfe_flankerDat.csv" # each output csv file will have this on its filename
surprise_csv_fileName <- "_mini_mfe_surpriseDat.csv" # each output csv file will have this on its filename


## creating a list of all raw data csv files in the input folder.
raw_datafiles_list <- c() # an empty list that will be filled in the next "for" loop!
csvSelect <- list.files(input_raw_path, pattern = ".csv") # listing only csv files
for (i in 1:length(csvSelect)){
  temp_for_file <- ifelse (str_detect(csvSelect[i], "face_flanker_v1", negate = FALSE), 1, 0)
  if (temp_for_file == 1){
    temp_list <- csvSelect[i]
    raw_datafiles_list <- c(raw_datafiles_list, temp_list)
  }
}

threshold_num <- 12 # threshold for the minimum number of incong errors
# Create an empty dataframe that will store two d' scores computed for each half. There will be 2 d' values for each participant. split_scores data frame
split_scores <- setNames(data.frame(matrix(ncol = 4, nrow = 0)), c("id","score_1","score_2", "replication"))
# Let's start a for loop that loops 100 times.

# Let's start a for loop that loops 100 times. (repeteation loop)
for (ww in 1:100){
  # Looping over all participants
  for (subject in 1:length(raw_datafiles_list)){
    #for this participant, find the raw csv file
    psychopy_file <- paste(input_raw_path,raw_datafiles_list[subject], sep = "/", collapse = NULL)

    #read in the data for this participant, establish id, and remove extraneous variables
    psychopyDat <- read.csv(file = psychopy_file, stringsAsFactors = FALSE, na.strings=c("", "NA"))
    participant_id <- psychopyDat$id[1]

    # Load this participant's flanker and surprise data frames
    flanker_name <- paste0(participant_id, flanker_csv_fileName, sep = "", collapse = NULL)
    surprise_name <- paste0(participant_id, surprise_csv_fileName, sep = "", collapse = NULL)
    flanker_df <- read.csv(file = paste(input_organized_path, flanker_name, sep = "/", collapse = NULL), stringsAsFactors = FALSE, na.strings=c("", "NA"))
    surprise_df <- read.csv(file = paste(input_organized_path, surprise_name, sep = "/", collapse = NULL), stringsAsFactors = FALSE, na.strings=c("", "NA"))

    # removing participants based on whether they just pressed the keys without actually paying attention to the task.
    # We check this by "keep_surp_memory_trial_based_on_rt" and "keep_surp_friendly_trial_based_on_rt" variables
    # in the surprise_df data frame. O means that they have responded faster than 200 ms during the given trial
    # and therefore, we need to remove that surprise trial.
    # In this study, we will remove people based on the surprise memory task not surprise friendly task.
    # If more than 20% of surprise trials are removed, we exclude that participant.
    number_of_removed_trials_in_the_memory_surp <- nrow(surprise_df) - (sum(surprise_df$keep_surp_memory_trial_based_on_rt))
    number_of_faces_in_surp_memory_task <- nrow(surprise_df)
    twenty_percent_threshold <- round(0.2 * number_of_faces_in_surp_memory_task)

    if (number_of_removed_trials_in_the_memory_surp <= twenty_percent_threshold){ # Participants who have less than twenty_percent_threshold surprise
      # trials removed, will be included.

      # Check to see if this participant has the flanker accuracy above 60%
      if (mean(as.numeric(flanker_df$current_trial_accuracy)) >= 0.6){

        # check to see if the participant has at least 'threshold_num' legit incongruent errors or not.
        incong_flankerDat <- filter(flanker_df, current_trial_congruency == 0)
        cong_flankerDat <- filter(flanker_df, current_trial_congruency == 1)
        error_incong_flankerDat <- filter(incong_flankerDat, current_trial_accuracy == 0)
        legit_error_incong_flankerDat <- filter(error_incong_flankerDat, current_trial_legitResponse == 1)
        if (nrow(legit_error_incong_flankerDat) >= threshold_num ){
          # Checking to see if there are at least 6 incongruent error faces in the surprise_df of this participant.
          # First we need to remove the trials that are marked based on rt!
          surprise_df <- filter(surprise_df, keep_surp_memory_trial_based_on_rt == 1)
          num_incong_error_faces_in_surp <- 0
          # Counting the number of legit incong error faces available in surprise_df!
          for (rr in 1:nrow(legit_error_incong_flankerDat)){
            temp_face <- legit_error_incong_flankerDat$current_trial_face[rr]
            temp_for_surp <- filter(surprise_df, face == temp_face)
            errorFace_exist_in_surpDat <- ifelse(nrow(temp_for_surp) == 1, 1,0)
            if (errorFace_exist_in_surpDat == 1){
              num_incong_error_faces_in_surp <- num_incong_error_faces_in_surp + 1
            }
          } # Closing the loop that counts the number of incong error faces available in surprise_df!
          if (num_incong_error_faces_in_surp >= threshold_num){

            # correct faces will be removed from surpDat.
            # A new column named 'condition' will be added to surpDat. 0 will indicate that the face is new and 1 will indicate that the face is old.
            # keep the face == 0 means it is a correct face.
            # keep the face == 1 means it is an error face.
            # keep the face == 2 means it is a new face.
            for (ppp in 1:nrow(surprise_df)){
              temp_cut_from_surp <- surprise_df$face[ppp]
              temp_cut_from_flanker <- filter(flanker_df, current_trial_face == temp_cut_from_surp)
              if (surprise_df$is_new[ppp] == 0){ # The face is old
                surprise_df$condition[ppp] <- 1
                if (temp_cut_from_flanker$current_trial_accuracy == 0){
                  surprise_df$keep_the_face[ppp] <- 1 # the face is coming from an error trial. So, we will keep it.
                } else if (temp_cut_from_flanker$current_trial_accuracy == 1){
                  surprise_df$keep_the_face[ppp] <- 0 # the face is coming from a correct trial.
                }
              } else if (surprise_df$is_new[ppp] == 1){ # The face is new
                surprise_df$condition[ppp] <- 0
                surprise_df$keep_the_face[ppp] <- 2 # New faces will also be kept and used.
              }
            }


            # A new column named 'surp_resp_acc' will be added to surpDat. 1 will indicate that the face is identified correctly and 0 will indicate
            # that the face is identified incorrectly.
            for (uuu in 1:nrow(surprise_df)){
              temp_cut_from_surp <- surprise_df$face[uuu]
              temp_cut_from_flanker <- filter(flanker_df, current_trial_face == temp_cut_from_surp)
              if (surprise_df$is_new[uuu] == surprise_df$identified_as_new[uuu]){
                surprise_df$surp_resp_acc[uuu] <- 1
              } else if (surprise_df$is_new[uuu] != surprise_df$identified_as_new[uuu]){
                surprise_df$surp_resp_acc[uuu] <- 0
              }
            }

          # To compute hitRate, only error faces will be selected.
          error_surpDat <- filter(surprise_df, keep_the_face == 1 )
          error_surpDat <- error_surpDat[sample(nrow(error_surpDat), threshold_num), ]
          split_size <- 0.5 * threshold_num
          first_half_for_split_error_surpDat <- error_surpDat[sample(nrow(error_surpDat), split_size), ]
          second_half_for_split_error_surpDat <- error_surpDat[!(error_surpDat$face %in% first_half_for_split_error_surpDat$face),]


          first_half_split <- first_half_for_split_error_surpDat
          first_half_split <- first_half_split[sample(1:nrow(first_half_split)), ] # shuffling rows randomly
          first_half_split <- subset(first_half_split, select = c(participant_id, condition, surp_resp_acc))

          ###### Compute hit Rate for the first half. This actually computes score_1 for this participant in split_scores.
          first_n_hit <- sum(first_half_split$condition == 1 & first_half_split$surp_resp_acc == 1) # Condition: 0 = new; 1 = Old
          first_n_miss <- sum(first_half_split$condition == 1 & first_half_split$surp_resp_acc == 0)
          score_1 <- (first_n_hit) / ((first_n_hit) + first_n_miss) # hit rate

          #######

          second_half_split <- second_half_for_split_error_surpDat
          second_half_split <- second_half_split[sample(1:nrow(second_half_split)), ] # shuffling rows randomly
          second_half_split <- subset(second_half_split, select = c(participant_id, condition, surp_resp_acc))

          ###### Compute d' for the second half. This actually computes score_2 for this participant in split_scores.
          second_n_hit <- sum(second_half_split$condition == 1 & second_half_split$surp_resp_acc == 1) # Condition: 0 = new; 1 = Old
          second_n_miss <- sum(second_half_split$condition == 1 & second_half_split$surp_resp_acc == 0)
          score_2 <- (second_n_hit) / ((second_n_hit) + second_n_miss) # hit rate

          #######
          replication <- ww
          split_scores[nrow(split_scores) + 1,] <-c(participant_id, score_1, score_2, replication)
          } # threshold_num faces in surp_dat
        } # threshold_num flanker errors
      } # accuracy check condition
    } # Less than 20% of surprise trials removed condition
  } # Closing the loop for each participant
} # Closing the repeteation loop

# Spearman-Brown adjusted Pearson correlations per replication
write.csv(split_scores, paste(output_path, "split_scores_hitRate_12.csv", sep = "/", collapse = NULL), row.names=FALSE)

coefs_per_round <- split_coefs(split_scores, spearman_brown)
write.csv(coefs_per_round, paste(output_path, "coefs_per_round_hitRate_12.csv", sep = "/", collapse = NULL), row.names=FALSE)

# Mean of coefficients
avg_coefs_per_round <- mean(coefs_per_round)
write.csv(avg_coefs_per_round, paste(output_path, "avg_coefs_per_round_hitRate_12.csv", sep = "/", collapse = NULL), row.names=FALSE)

